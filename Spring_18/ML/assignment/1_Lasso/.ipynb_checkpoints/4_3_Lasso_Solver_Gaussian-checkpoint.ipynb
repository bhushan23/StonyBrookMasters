{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fDF = pd.read_csv('Data/featureTypes.txt', names=['featureID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1)\n",
      "flavors raspberries cherries\n"
     ]
    }
   ],
   "source": [
    "print fDF.shape\n",
    "print fDF['featureID'][0]\n",
    "n = 10000\n",
    "d = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247847\n"
     ]
    }
   ],
   "source": [
    "trainDF = pd.read_csv('Data/trainData.txt', names = ['instanceID', 'featureID', 'value'], sep=' ')\n",
    "YDF = pd.read_csv('Data/trainLabels.txt', names = ['label'])\n",
    "valXDF = pd.read_csv('Data/valData.txt', names = ['instanceID', 'featureID', 'value'], sep=' ')\n",
    "valYDF = pd.read_csv('Data/valLabels.txt', names = ['label'])\n",
    "print trainDF.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000,)\n",
      "3000\n",
      "3000\n",
      "[ 0.51067864  0.89935112  0.59453212 ...,  0.51700815  0.33900953\n",
      "  0.52986774]\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(d)  #random.uniform(low=0.0, high=1.0,size = (d,))\n",
    "B = 0 #np.zeros(n)\n",
    "TempBArray = np.ones(n)\n",
    "print W.shape\n",
    "#print TempBArray\n",
    "print np.count_nonzero(W)\n",
    "print len(W)\n",
    "print W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instanceID</th>\n",
       "      <th>featureID</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>228</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>242</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>371</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instanceID  featureID  value\n",
       "0           1         13  0.209\n",
       "1           1         83  0.209\n",
       "2           1        228  0.209\n",
       "3           1        242  0.209\n",
       "4           1        371  0.209"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tDF = csr_matrix(trainDF) \n",
    "#print tDF[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sdf = pd.SparseDataFrame(tDF)\n",
    "#print sdf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will lead to negative index if re-running\n",
    "trainDF['instanceID'] -= 1\n",
    "trainDF['featureID'] -= 1\n",
    "sMat = csr_matrix((trainDF['value'], (trainDF['featureID'], trainDF['instanceID'])))\n",
    "valXDF['instanceID'] -= 1\n",
    "valXDF['featureID'] -= 1\n",
    "valX = csr_matrix((valXDF['value'], (valXDF['featureID'], valXDF['instanceID'])))\n",
    "Y = YDF['label'].as_matrix().transpose()\n",
    "#print Y.shape\n",
    "valY = valYDF['label'].as_matrix().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728 0.0\n"
     ]
    }
   ],
   "source": [
    "#print sMat.shape\n",
    "#print sMat.todense()\n",
    "print sMat.max(), sMat.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print sMat[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 10000)\n"
     ]
    }
   ],
   "source": [
    "X = sMat.copy()\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "print len(W.nonzero())\n",
    "print len(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1271.869651\n"
     ]
    }
   ],
   "source": [
    "def initLamda(X, Y):\n",
    "    YNorm = Y - float(Y.sum())/((float)(Y.shape[0]))\n",
    "    return 2 * (abs(X * YNorm).max())\n",
    "    \n",
    "print initLamda(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tX = X.copy()\n",
    "#print tX\n",
    "#tX.data **= 2\n",
    "#tA = 2*tX.sum(axis = 1)\n",
    "#print max(tA), min(tA)\n",
    "#print tA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to convergence condition\n",
    "\n",
    "def rmse(input1, input2):\n",
    "    out = input1 - input2\n",
    "    #print out\n",
    "    out **= 2\n",
    "    out /= len(out)\n",
    "    error = out.sum()\n",
    "    return math.sqrt(error)\n",
    "\n",
    "\n",
    "class Lasso:\n",
    "    def __init__(self, X, Y, W, B, Lamda):\n",
    "        self.X = X.copy()\n",
    "        self.Y = Y.copy()\n",
    "        self.W = W #.copy()  # Remove this copy later\n",
    "        self.B = B #.copy()\n",
    "        t = X.copy()\n",
    "        t.data **= 2\n",
    "        self.A = 2*t.sum(axis = 1)\n",
    "        self.Lamda = Lamda\n",
    "        self.delta = 0.0001\n",
    "        # Stores Lamda and respective RMSE\n",
    "        self.trainrmse = []\n",
    "        self.trainlamda = []\n",
    "        self.valrmse = []\n",
    "        self.vallamda = []\n",
    "        self.NonZero = []\n",
    "        \n",
    "    def loss(self):\n",
    "        return ((self.X.transpose() * self.W + self.B - self.Y) ** 2).sum() + self.Lamda * (abs(self.W)).sum()\n",
    "        \n",
    "    def fit(self):\n",
    "        # Lamda = initLamda(self.X, self.Y)\n",
    "        \n",
    "        #print X.shape, W.shape\n",
    "        #for epoch in range(100):\n",
    "        oldLoss = self.loss()+2\n",
    "        newLoss = self.loss()\n",
    "        print 'Lamda: ', self.Lamda\n",
    "        while oldLoss - newLoss > self.delta:\n",
    "        #for i in range(1000):\n",
    "            #print sMat.transpose() * W\n",
    "            # 4.1.1\n",
    "            #print t1[:5], t1.shape\n",
    "            #print t1.shape, B.shape, Y.shape\n",
    "            XTW = (self.X.transpose() * self.W)\n",
    "            R = self.Y - (self.X.transpose() * self.W) - self.B\n",
    "            \n",
    "            # 4.1.2\n",
    "            BOld = self.B\n",
    "            self.B = np.full(n, (R + self.B).sum() / n) \n",
    "            #print self.B\n",
    "            #print self.B.shape\n",
    "            #self.B = (self.Y - XTW).sum() / n\n",
    "            #print B.shape\n",
    "            # 4.1.3\n",
    "            R =  R + BOld - self.B\n",
    "            #print 'RSHAPE', R.shape\n",
    "            #print 'R', R\n",
    "            #R = self.Y - (XTW + self.B)\n",
    "            #print R.shape\n",
    "            #print R[:5]\n",
    "            # R = R.reshape(-1)\n",
    "            for ik in range(0, d):\n",
    "                # 4.1.4\n",
    "                #ik = 0\n",
    "                t = (self.X[ik].transpose() * self.W[ik]).toarray().reshape(-1)\n",
    "                #print t\n",
    "                #print t.shape\n",
    "                #print R.shape\n",
    "                Ck = 2*( self.X[ik] * (R + t)).sum()\n",
    "                #print 'CK:', Ck\n",
    "                # Update Weight\n",
    "                WkOld = self.W[ik]\n",
    "                #print 'OW: ', WkOld\n",
    "                if Ck < -self.Lamda:\n",
    "                    self.W[ik] = (Ck + self.Lamda) / self.A[ik]\n",
    "                elif Ck > self.Lamda:\n",
    "                    self.W[ik] = (Ck - self.Lamda) / self.A[ik]\n",
    "                else:\n",
    "                    self.W[ik] = 0\n",
    "                #print 'W: ', WkOld, self.W[ik]\n",
    "                #print W[ik]\n",
    "                # 4.1.5\n",
    "                # print self.W[ik], WkOld\n",
    "                #print X[ik].toarray().reshape(-1).shape, R.shape\n",
    "                R = R + self.X[ik].toarray().reshape(-1) * (WkOld - self.W[ik])\n",
    "                #R = self.Y - (self.X.transpose() * self.W) + self.B\n",
    "            oldLoss = newLoss\n",
    "            newLoss = model.loss()\n",
    "            #print oldLoss, newLoss, oldLoss - newLoss\n",
    "            print 'LOSS:' , newLoss\n",
    "            # End of feature vector iterator\n",
    "    \n",
    "    def saveModel(self, filename):\n",
    "        pickle.dump(self, open( filename, \"wb\" ))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (X.transpose() * self.W + np.full(X.transpose().shape[0], self.B))\n",
    "    \n",
    "    def chooseCorrectLamda(self, delta = -1):\n",
    "        oldLamda = self.Lamda\n",
    "        if delta != -1:\n",
    "            self.delta = delta\n",
    "        self.fit()\n",
    "        \n",
    "        newRMSE = rmse(self.predict(self.X), self.Y)\n",
    "        #self.TrainInfo.append([self.Lamda, newRMSE])\n",
    "        self.trainrmse.append(newRMSE)\n",
    "        self.trainlamda.append(self.Lamda)\n",
    "        valRMSE = rmse(self.predict(valX), valY)\n",
    "        self.valrmse.append(valRMSE)\n",
    "        self.vallamda.append(self.Lamda)\n",
    "        oldRMSE = valRMSE\n",
    "        #print W\n",
    "        #self.ValInfo.append([self.Lamda, valRMSE])\n",
    "        self.NonZero.append((self.W != 0.0).sum())\n",
    "        print 'Lamda: ', self.Lamda, 'RMSE: ', newRMSE, 'Val RMSE:' , valRMSE\n",
    "        \n",
    "        while oldRMSE >= valRMSE:\n",
    "            oldLamda = self.Lamda\n",
    "            self.Lamda /= 2\n",
    "            self.fit()\n",
    "            oldRMSE = valRMSE\n",
    "            #self.TrainInfo.append([self.Lamda, newRMSE])\n",
    "            newRMSE = rmse(self.predict(self.X), self.Y)\n",
    "            self.trainrmse.append(newRMSE)\n",
    "            self.trainlamda.append(self.Lamda)\n",
    "            valRMSE = rmse(self.predict(valX), valY)\n",
    "            #self.ValInfo.append([self.Lamda, valRMSE])\n",
    "            self.valrmse.append(valRMSE)\n",
    "            self.vallamda.append(self.Lamda)\n",
    "            self.NonZero.append(np.count_nonzero(self.W))\n",
    "            #self.NonZero.append(self.W.toarray().count_nonzero())\n",
    "            print 'Lamda: ', self.Lamda, 'RMSE: ', newRMSE, 'Val RMSE:' , valRMSE\n",
    "            self.saveModel('optimal_saved_Model')\n",
    "    \n",
    "def loadModel(filename):\n",
    "    return pickle.load(open(filename, \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(X, Y, W, B,initLamda(X.copy(), Y.copy()))\n",
    "#model.loss()\n",
    "#model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1271.869651\n"
     ]
    }
   ],
   "source": [
    "print model.Lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  1271.869651\n",
      "LOSS: 153909.337446\n",
      "LOSS: 125084.486188\n",
      "LOSS: 115841.375741\n",
      "LOSS: 115339.9759\n",
      "LOSS: 115339.9759\n",
      "Lamda:  1271.869651 RMSE:  3.39617396345 Val RMSE: 3.38958619451\n",
      "Lamda:  635.9348255\n",
      "LOSS: 113464.97052\n",
      "LOSS: 113351.724815\n",
      "LOSS: 113268.054226\n",
      "LOSS: 113230.355473\n",
      "LOSS: 113218.396063\n",
      "LOSS: 113214.759373\n",
      "LOSS: 113213.598487\n",
      "LOSS: 113213.216459\n",
      "LOSS: 113213.088791\n",
      "LOSS: 113213.045865\n",
      "LOSS: 113213.031406\n",
      "LOSS: 113213.026535\n",
      "LOSS: 113213.024894\n",
      "LOSS: 113213.024341\n",
      "LOSS: 113213.024154\n",
      "LOSS: 113213.024092\n",
      "Lamda:  635.9348255 RMSE:  3.23820937646 Val RMSE: 3.24103842561\n",
      "Lamda:  317.96741275\n",
      "LOSS: 104745.174232\n",
      "LOSS: 104554.531916\n",
      "LOSS: 104545.441437\n",
      "LOSS: 104541.907392\n",
      "LOSS: 104540.439633\n",
      "LOSS: 104539.826435\n",
      "LOSS: 104539.567779\n",
      "LOSS: 104539.457179\n",
      "LOSS: 104539.409135\n",
      "LOSS: 104539.387981\n",
      "LOSS: 104539.378584\n",
      "LOSS: 104539.374393\n",
      "LOSS: 104539.372521\n",
      "LOSS: 104539.371685\n",
      "LOSS: 104539.371311\n",
      "LOSS: 104539.371144\n",
      "LOSS: 104539.37107\n",
      "Lamda:  317.96741275 RMSE:  2.9874621538 Val RMSE: 3.00146777552\n",
      "Lamda:  158.983706375\n",
      "LOSS: 93411.8626852\n",
      "LOSS: 93142.5927483\n",
      "LOSS: 93116.2798165\n",
      "LOSS: 93094.5381255\n",
      "LOSS: 93076.7436093\n",
      "LOSS: 93064.1825668\n",
      "LOSS: 93056.4346507\n",
      "LOSS: 93052.1362785\n",
      "LOSS: 93049.9123546\n",
      "LOSS: 93048.8025955\n",
      "LOSS: 93048.2552723\n",
      "LOSS: 93047.9838168\n",
      "LOSS: 93047.8472207\n",
      "LOSS: 93047.7778996\n",
      "LOSS: 93047.7425349\n",
      "LOSS: 93047.7243966\n",
      "LOSS: 93047.7150447\n",
      "LOSS: 93047.7102035\n",
      "LOSS: 93047.7076913\n",
      "LOSS: 93047.7063863\n",
      "LOSS: 93047.7057084\n",
      "LOSS: 93047.7053563\n",
      "LOSS: 93047.7051735\n",
      "LOSS: 93047.7050786\n",
      "Lamda:  158.983706375 RMSE:  2.75315029849 Val RMSE: 2.78489374581\n",
      "Lamda:  79.4918531875\n",
      "LOSS: 81018.7736378\n",
      "LOSS: 80575.3021985\n",
      "LOSS: 80524.669405\n",
      "LOSS: 80492.660703\n",
      "LOSS: 80466.9761894\n",
      "LOSS: 80445.0270725\n",
      "LOSS: 80426.1500299\n",
      "LOSS: 80410.2239926\n",
      "LOSS: 80397.5636122\n",
      "LOSS: 80387.5829308\n",
      "LOSS: 80379.486391\n",
      "LOSS: 80373.3313957\n",
      "LOSS: 80368.9007815\n",
      "LOSS: 80365.8196127\n",
      "LOSS: 80363.7144809\n",
      "LOSS: 80362.2683564\n",
      "LOSS: 80361.2665229\n",
      "LOSS: 80360.5665781\n",
      "LOSS: 80360.0739058\n",
      "LOSS: 80359.7251672\n",
      "LOSS: 80359.4773782\n",
      "LOSS: 80359.3009465\n",
      "LOSS: 80359.1752263\n",
      "LOSS: 80359.0856237\n",
      "LOSS: 80359.021854\n",
      "LOSS: 80358.9765828\n",
      "LOSS: 80358.9444912\n",
      "LOSS: 80358.9217607\n",
      "LOSS: 80358.9056583\n",
      "LOSS: 80358.8942447\n",
      "LOSS: 80358.8861497\n",
      "LOSS: 80358.8804064\n",
      "LOSS: 80358.8763309\n",
      "LOSS: 80358.873439\n",
      "LOSS: 80358.8713871\n",
      "LOSS: 80358.8699315\n",
      "LOSS: 80358.868899\n",
      "LOSS: 80358.8681666\n",
      "LOSS: 80358.8676472\n",
      "LOSS: 80358.8672788\n",
      "LOSS: 80358.8670176\n",
      "LOSS: 80358.8668323\n",
      "LOSS: 80358.8667009\n",
      "LOSS: 80358.8666077\n",
      "Lamda:  79.4918531875 RMSE:  2.48629664111 Val RMSE: 2.54088742587\n",
      "Lamda:  39.7459265938\n",
      "LOSS: 68631.711069\n",
      "LOSS: 68272.2606971\n",
      "LOSS: 68234.6316947\n",
      "LOSS: 68210.8761184\n",
      "LOSS: 68188.9157642\n",
      "LOSS: 68168.348572\n",
      "LOSS: 68149.2613247\n",
      "LOSS: 68131.439185\n",
      "LOSS: 68114.6606888\n",
      "LOSS: 68098.7867301\n",
      "LOSS: 68084.2301383\n",
      "LOSS: 68071.4079637\n",
      "LOSS: 68060.5247794\n",
      "LOSS: 68051.5681257\n",
      "LOSS: 68044.3684652\n",
      "LOSS: 68038.6640577\n",
      "LOSS: 68034.1239687\n",
      "LOSS: 68030.4978434\n",
      "LOSS: 68027.580603\n",
      "LOSS: 68025.2149624\n",
      "LOSS: 68023.2819311\n",
      "LOSS: 68021.6912963\n",
      "LOSS: 68020.3703201\n",
      "LOSS: 68019.2667154\n",
      "LOSS: 68018.3421737\n",
      "LOSS: 68017.5665264\n",
      "LOSS: 68016.9156422\n",
      "LOSS: 68016.3695084\n",
      "LOSS: 68015.9115024\n",
      "LOSS: 68015.5277256\n",
      "LOSS: 68015.2064747\n",
      "LOSS: 68014.9377973\n",
      "LOSS: 68014.712306\n",
      "LOSS: 68014.523458\n",
      "LOSS: 68014.3654885\n",
      "LOSS: 68014.2334778\n",
      "LOSS: 68014.1232417\n",
      "LOSS: 68014.031231\n",
      "LOSS: 68013.9544478\n",
      "LOSS: 68013.890372\n",
      "LOSS: 68013.8368945\n",
      "LOSS: 68013.792255\n",
      "LOSS: 68013.7549871\n",
      "LOSS: 68013.7238697\n",
      "LOSS: 68013.6978854\n",
      "LOSS: 68013.6761868\n",
      "LOSS: 68013.6580667\n",
      "LOSS: 68013.6429351\n",
      "LOSS: 68013.6302996\n",
      "LOSS: 68013.6197486\n",
      "LOSS: 68013.6109386\n",
      "LOSS: 68013.6035824\n",
      "LOSS: 68013.5974403\n",
      "LOSS: 68013.592312\n",
      "LOSS: 68013.5880301\n",
      "LOSS: 68013.584455\n",
      "LOSS: 68013.58147\n",
      "LOSS: 68013.5789777\n",
      "LOSS: 68013.5768967\n",
      "LOSS: 68013.5751592\n",
      "LOSS: 68013.5737085\n",
      "LOSS: 68013.5724972\n",
      "LOSS: 68013.5714858\n",
      "LOSS: 68013.5706413\n",
      "LOSS: 68013.5699362\n",
      "LOSS: 68013.5693474\n",
      "LOSS: 68013.5688559\n",
      "LOSS: 68013.5684454\n",
      "LOSS: 68013.5681027\n",
      "LOSS: 68013.5678166\n",
      "LOSS: 68013.5675776\n",
      "LOSS: 68013.5673781\n",
      "LOSS: 68013.5672116\n",
      "LOSS: 68013.5670725\n",
      "LOSS: 68013.5669564\n",
      "LOSS: 68013.5668594\n",
      "Lamda:  39.7459265938 RMSE:  2.26492297566 Val RMSE: 2.34252765276\n",
      "Lamda:  19.8729632969\n",
      "LOSS: 57828.563022\n",
      "LOSS: 57558.4224828\n",
      "LOSS: 57523.3924963\n",
      "LOSS: 57504.9196088\n",
      "LOSS: 57488.7790143\n",
      "LOSS: 57473.4246926\n",
      "LOSS: 57458.667904\n",
      "LOSS: 57444.5259684\n",
      "LOSS: 57430.9903022\n",
      "LOSS: 57418.0409032\n",
      "LOSS: 57405.6844485\n",
      "LOSS: 57393.955415\n",
      "LOSS: 57382.9465862\n",
      "LOSS: 57372.9353012\n",
      "LOSS: 57364.0403088\n",
      "LOSS: 57356.2666161\n",
      "LOSS: 57349.5508806\n",
      "LOSS: 57343.800491\n",
      "LOSS: 57338.901708\n",
      "LOSS: 57334.7414097\n",
      "LOSS: 57331.2104917\n",
      "LOSS: 57328.2025715\n",
      "LOSS: 57325.6266671\n",
      "LOSS: 57323.3987442\n",
      "LOSS: 57321.4561111\n",
      "LOSS: 57319.7505345\n",
      "LOSS: 57318.2458087\n",
      "LOSS: 57316.9159713\n",
      "LOSS: 57315.7409865\n",
      "LOSS: 57314.7133073\n",
      "LOSS: 57313.8082591\n",
      "LOSS: 57313.0125481\n",
      "LOSS: 57312.3142851\n",
      "LOSS: 57311.7026052\n",
      "LOSS: 57311.1671911\n",
      "LOSS: 57310.6976214\n",
      "LOSS: 57310.2860087\n",
      "LOSS: 57309.9255082\n",
      "LOSS: 57309.6100688\n",
      "LOSS: 57309.3342451\n",
      "LOSS: 57309.0931321\n",
      "LOSS: 57308.882315\n",
      "LOSS: 57308.6979113\n",
      "LOSS: 57308.5365105\n",
      "LOSS: 57308.3951378\n",
      "LOSS: 57308.2712098\n",
      "LOSS: 57308.162487\n",
      "LOSS: 57308.0670296\n",
      "LOSS: 57307.9831563\n",
      "LOSS: 57307.9094088\n",
      "LOSS: 57307.8445203\n",
      "LOSS: 57307.7873887\n",
      "LOSS: 57307.7370538\n",
      "LOSS: 57307.6926773\n",
      "LOSS: 57307.6535268\n",
      "LOSS: 57307.6189613\n",
      "LOSS: 57307.5884198\n",
      "LOSS: 57307.5614108\n",
      "LOSS: 57307.5375036\n",
      "LOSS: 57307.5163235\n",
      "LOSS: 57307.4975387\n",
      "LOSS: 57307.4807627\n",
      "LOSS: 57307.46577\n",
      "LOSS: 57307.4523718\n",
      "LOSS: 57307.4403975\n",
      "LOSS: 57307.4297123\n",
      "LOSS: 57307.420139\n",
      "LOSS: 57307.4114944\n",
      "LOSS: 57307.4036439\n",
      "LOSS: 57307.3964992\n",
      "LOSS: 57307.3899958\n",
      "LOSS: 57307.3840786\n",
      "LOSS: 57307.3786949\n",
      "LOSS: 57307.373794\n",
      "LOSS: 57307.3693277\n",
      "LOSS: 57307.3652513\n",
      "LOSS: 57307.361524\n",
      "LOSS: 57307.3581087\n",
      "LOSS: 57307.3549722\n",
      "LOSS: 57307.3520847\n",
      "LOSS: 57307.3494199\n",
      "LOSS: 57307.3469542\n",
      "LOSS: 57307.3446665\n",
      "LOSS: 57307.3425384\n",
      "LOSS: 57307.3405534\n",
      "LOSS: 57307.3386968\n",
      "LOSS: 57307.3369559\n",
      "LOSS: 57307.335319\n",
      "LOSS: 57307.3337761\n",
      "LOSS: 57307.332318\n",
      "LOSS: 57307.3309368\n",
      "LOSS: 57307.3296251\n",
      "LOSS: 57307.3283767\n",
      "LOSS: 57307.3271858\n",
      "LOSS: 57307.3260474\n",
      "LOSS: 57307.3249568\n",
      "LOSS: 57307.32391\n",
      "LOSS: 57307.3229033\n",
      "LOSS: 57307.3219335\n",
      "LOSS: 57307.3209977\n",
      "LOSS: 57307.3200932\n",
      "LOSS: 57307.3192177\n",
      "LOSS: 57307.3183692\n",
      "LOSS: 57307.3177912\n",
      "LOSS: 57307.3177315\n",
      "Lamda:  19.8729632969 RMSE:  2.06996661086 Val RMSE: 2.18229530524\n",
      "Lamda:  9.93648164844\n",
      "LOSS: 48300.1674278\n",
      "LOSS: 48057.2527653\n",
      "LOSS: 48025.6154791\n",
      "LOSS: 48009.2065306\n",
      "LOSS: 47996.0535426\n",
      "LOSS: 47984.0352575\n",
      "LOSS: 47972.688087\n",
      "LOSS: 47961.85097\n",
      "LOSS: 47951.4495377\n",
      "LOSS: 47941.3618589\n",
      "LOSS: 47931.523903\n",
      "LOSS: 47921.916958\n",
      "LOSS: 47912.5579685\n",
      "LOSS: 47903.5144337\n",
      "LOSS: 47894.8572366\n",
      "LOSS: 47886.6563858\n",
      "LOSS: 47878.9285423\n",
      "LOSS: 47871.6911428\n",
      "LOSS: 47864.9688707\n",
      "LOSS: 47858.7870593\n",
      "LOSS: 47853.1553555\n",
      "LOSS: 47848.0601581\n",
      "LOSS: 47843.4666197\n",
      "LOSS: 47839.3413164\n",
      "LOSS: 47835.6446887\n",
      "LOSS: 47832.3315995\n",
      "LOSS: 47829.3556027\n",
      "LOSS: 47826.6793059\n",
      "LOSS: 47824.2671243\n",
      "LOSS: 47822.0876622\n",
      "LOSS: 47820.1130567\n",
      "LOSS: 47818.319113\n",
      "LOSS: 47816.6851972\n",
      "LOSS: 47815.1937251\n",
      "LOSS: 47813.8300775\n",
      "LOSS: 47812.5816849\n",
      "LOSS: 47811.4379235\n",
      "LOSS: 47810.3891499\n",
      "LOSS: 47809.4270764\n",
      "LOSS: 47808.5444306\n",
      "LOSS: 47807.7338082\n",
      "LOSS: 47806.9875041\n",
      "LOSS: 47806.302565\n",
      "LOSS: 47805.6743371\n",
      "LOSS: 47805.0968371\n",
      "LOSS: 47804.5674061\n",
      "LOSS: 47804.0830066\n",
      "LOSS: 47803.6404444\n",
      "LOSS: 47803.2361275\n",
      "LOSS: 47802.86681\n",
      "LOSS: 47802.5289235\n",
      "LOSS: 47802.2194345\n",
      "LOSS: 47801.9357421\n",
      "LOSS: 47801.6756185\n",
      "LOSS: 47801.4370554\n",
      "LOSS: 47801.218259\n",
      "LOSS: 47801.0176672\n",
      "LOSS: 47800.8338364\n",
      "LOSS: 47800.6654189\n",
      "LOSS: 47800.5111688\n",
      "LOSS: 47800.3699247\n",
      "LOSS: 47800.2405887\n",
      "LOSS: 47800.1221247\n",
      "LOSS: 47800.013564\n",
      "LOSS: 47799.9140252\n",
      "LOSS: 47799.8227104\n",
      "LOSS: 47799.7388996\n",
      "LOSS: 47799.6619418\n",
      "LOSS: 47799.5912476\n",
      "LOSS: 47799.5262821\n",
      "LOSS: 47799.4709352\n",
      "LOSS: 47799.4223857\n",
      "LOSS: 47799.3780274\n",
      "LOSS: 47799.337498\n",
      "LOSS: 47799.3004578\n",
      "LOSS: 47799.2665955\n",
      "LOSS: 47799.2356293\n",
      "LOSS: 47799.2073041\n",
      "LOSS: 47799.1813881\n",
      "LOSS: 47799.1576699\n",
      "LOSS: 47799.135957\n",
      "LOSS: 47799.1160735\n",
      "LOSS: 47799.0978593\n",
      "LOSS: 47799.0811692\n",
      "LOSS: 47799.0658703\n",
      "LOSS: 47799.0518413\n",
      "LOSS: 47799.0389719\n",
      "LOSS: 47799.0271613\n",
      "LOSS: 47799.0163179\n",
      "LOSS: 47799.0063581\n",
      "LOSS: 47798.9972058\n",
      "LOSS: 47798.9887916\n",
      "LOSS: 47798.9810524\n",
      "LOSS: 47798.9739305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 47798.9673733\n",
      "LOSS: 47798.961333\n",
      "LOSS: 47798.9557658\n",
      "LOSS: 47798.9506318\n",
      "LOSS: 47798.9458944\n",
      "LOSS: 47798.9415205\n",
      "LOSS: 47798.9374797\n",
      "LOSS: 47798.9337443\n",
      "LOSS: 47798.9302889\n",
      "LOSS: 47798.9270904\n",
      "LOSS: 47798.9241278\n",
      "LOSS: 47798.9213816\n",
      "LOSS: 47798.9188345\n",
      "LOSS: 47798.9164704\n",
      "LOSS: 47798.9142743\n",
      "LOSS: 47798.9122326\n",
      "LOSS: 47798.9103329\n",
      "LOSS: 47798.908564\n",
      "LOSS: 47798.9069157\n",
      "LOSS: 47798.9053784\n",
      "LOSS: 47798.9039436\n",
      "LOSS: 47798.9026035\n",
      "LOSS: 47798.9013507\n",
      "LOSS: 47798.9001788\n",
      "LOSS: 47798.8990816\n",
      "LOSS: 47798.8980535\n",
      "LOSS: 47798.8970894\n",
      "LOSS: 47798.8961846\n",
      "LOSS: 47798.8953348\n",
      "LOSS: 47798.894536\n",
      "LOSS: 47798.8937845\n",
      "LOSS: 47798.8930771\n",
      "LOSS: 47798.8924106\n",
      "LOSS: 47798.8917823\n",
      "LOSS: 47798.8911895\n",
      "LOSS: 47798.8906298\n",
      "LOSS: 47798.890101\n",
      "LOSS: 47798.8896012\n",
      "LOSS: 47798.8891283\n",
      "LOSS: 47798.8886807\n",
      "LOSS: 47798.8882568\n",
      "LOSS: 47798.887855\n",
      "LOSS: 47798.8874741\n",
      "LOSS: 47798.8871127\n",
      "LOSS: 47798.8867696\n",
      "LOSS: 47798.8864438\n",
      "LOSS: 47798.8861342\n",
      "LOSS: 47798.8858398\n",
      "LOSS: 47798.8855598\n",
      "LOSS: 47798.8852934\n",
      "LOSS: 47798.8850398\n",
      "LOSS: 47798.8847983\n",
      "LOSS: 47798.8845682\n",
      "LOSS: 47798.8843489\n",
      "LOSS: 47798.8841397\n",
      "LOSS: 47798.8839403\n",
      "LOSS: 47798.8837499\n",
      "LOSS: 47798.8835683\n",
      "LOSS: 47798.8833949\n",
      "LOSS: 47798.8832293\n",
      "LOSS: 47798.883071\n",
      "LOSS: 47798.8829199\n",
      "LOSS: 47798.8827754\n",
      "LOSS: 47798.8826372\n",
      "LOSS: 47798.8825052\n",
      "LOSS: 47798.8823788\n",
      "LOSS: 47798.882258\n",
      "LOSS: 47798.8821423\n",
      "LOSS: 47798.8820317\n",
      "LOSS: 47798.8819258\n",
      "LOSS: 47798.8818244\n",
      "LOSS: 47798.8817273\n",
      "Lamda:  9.93648164844 RMSE:  1.86831302962 Val RMSE: 2.04547890162\n",
      "Lamda:  4.96824082422\n",
      "LOSS: 40029.2884097\n",
      "LOSS: 39858.5546726\n",
      "LOSS: 39829.7134167\n",
      "LOSS: 39817.9394501\n",
      "LOSS: 39809.7705933\n",
      "LOSS: 39802.4985534\n",
      "LOSS: 39795.6291003\n",
      "LOSS: 39789.0074507\n",
      "LOSS: 39782.5433325\n",
      "LOSS: 39776.2682223\n",
      "LOSS: 39770.1460129\n",
      "LOSS: 39764.1571093\n",
      "LOSS: 39758.3055176\n",
      "LOSS: 39752.6020391\n",
      "LOSS: 39747.0681264\n",
      "LOSS: 39741.6858\n",
      "LOSS: 39736.4807502\n",
      "LOSS: 39731.4665766\n",
      "LOSS: 39726.6767329\n",
      "LOSS: 39722.1073347\n",
      "LOSS: 39717.7594558\n",
      "LOSS: 39713.6439884\n",
      "LOSS: 39709.764728\n",
      "LOSS: 39706.1215446\n",
      "LOSS: 39702.7114136\n",
      "LOSS: 39699.529497\n",
      "LOSS: 39696.5669032\n",
      "LOSS: 39693.8117688\n",
      "LOSS: 39691.251249\n",
      "LOSS: 39688.8699009\n",
      "LOSS: 39686.6552589\n",
      "LOSS: 39684.5978712\n",
      "LOSS: 39682.6858056\n",
      "LOSS: 39680.9069207\n",
      "LOSS: 39679.2503912\n",
      "LOSS: 39677.70648\n",
      "LOSS: 39676.2654619\n",
      "LOSS: 39674.9192594\n",
      "LOSS: 39673.6602303\n",
      "LOSS: 39672.4813991\n",
      "LOSS: 39671.3765203\n",
      "LOSS: 39670.3399613\n",
      "LOSS: 39669.3673158\n",
      "LOSS: 39668.4541097\n",
      "LOSS: 39667.5963965\n",
      "LOSS: 39666.7911534\n",
      "LOSS: 39666.0364175\n",
      "LOSS: 39665.331157\n",
      "LOSS: 39664.6729411\n",
      "LOSS: 39664.0588985\n",
      "LOSS: 39663.4861243\n",
      "LOSS: 39662.9518579\n",
      "LOSS: 39662.4534855\n",
      "LOSS: 39661.9885799\n",
      "LOSS: 39661.5547855\n",
      "LOSS: 39661.1500945\n",
      "LOSS: 39660.7726264\n",
      "LOSS: 39660.4206044\n",
      "LOSS: 39660.0923474\n",
      "LOSS: 39659.7864096\n",
      "LOSS: 39659.5011978\n",
      "LOSS: 39659.2351634\n",
      "LOSS: 39658.986953\n",
      "LOSS: 39658.7553563\n",
      "LOSS: 39658.5392608\n",
      "LOSS: 39658.3376303\n",
      "LOSS: 39658.1494933\n",
      "LOSS: 39657.973926\n",
      "LOSS: 39657.810069\n",
      "LOSS: 39657.6571224\n",
      "LOSS: 39657.5143202\n",
      "LOSS: 39657.3809486\n",
      "LOSS: 39657.2563456\n",
      "LOSS: 39657.1398981\n",
      "LOSS: 39657.0310339\n",
      "LOSS: 39656.9292283\n",
      "LOSS: 39656.8339996\n",
      "LOSS: 39656.7449055\n",
      "LOSS: 39656.6615388\n",
      "LOSS: 39656.5835243\n",
      "LOSS: 39656.5105152\n",
      "LOSS: 39656.44219\n",
      "LOSS: 39656.3782502\n",
      "LOSS: 39656.3184174\n",
      "LOSS: 39656.2624318\n",
      "LOSS: 39656.2100982\n",
      "LOSS: 39656.1611587\n",
      "LOSS: 39656.1153839\n",
      "LOSS: 39656.0725686\n",
      "LOSS: 39656.0325232\n",
      "LOSS: 39655.9950301\n",
      "LOSS: 39655.9599522\n",
      "LOSS: 39655.9271531\n",
      "LOSS: 39655.8965209\n",
      "LOSS: 39655.8679309\n",
      "LOSS: 39655.8412534\n",
      "LOSS: 39655.8163621\n",
      "LOSS: 39655.7931379\n",
      "LOSS: 39655.7714702\n",
      "LOSS: 39655.7512568\n",
      "LOSS: 39655.7324035\n",
      "LOSS: 39655.7148233\n",
      "LOSS: 39655.6984352\n",
      "LOSS: 39655.6831641\n",
      "LOSS: 39655.6689392\n",
      "LOSS: 39655.655694\n",
      "LOSS: 39655.6433656\n",
      "LOSS: 39655.6318944\n",
      "LOSS: 39655.6212238\n",
      "LOSS: 39655.6113002\n",
      "LOSS: 39655.602073\n",
      "LOSS: 39655.5934942\n",
      "LOSS: 39655.5855189\n",
      "LOSS: 39655.5781047\n",
      "LOSS: 39655.5712156\n",
      "LOSS: 39655.5648475\n",
      "LOSS: 39655.5589362\n",
      "LOSS: 39655.553444\n",
      "LOSS: 39655.5483388\n",
      "LOSS: 39655.5435912\n",
      "LOSS: 39655.5391749\n",
      "LOSS: 39655.5350657\n",
      "LOSS: 39655.5312415\n",
      "LOSS: 39655.5276823\n",
      "LOSS: 39655.5243695\n",
      "LOSS: 39655.5212864\n",
      "LOSS: 39655.5184172\n",
      "LOSS: 39655.5157473\n",
      "LOSS: 39655.5132632\n",
      "LOSS: 39655.5109524\n",
      "LOSS: 39655.5088028\n",
      "LOSS: 39655.5068037\n",
      "LOSS: 39655.5049445\n",
      "LOSS: 39655.5032157\n",
      "LOSS: 39655.5016083\n",
      "LOSS: 39655.5001139\n",
      "LOSS: 39655.4987245\n",
      "LOSS: 39655.4974329\n",
      "LOSS: 39655.4962322\n",
      "LOSS: 39655.495116\n",
      "LOSS: 39655.4940784\n",
      "LOSS: 39655.4931139\n",
      "LOSS: 39655.4922173\n",
      "LOSS: 39655.4913839\n",
      "LOSS: 39655.4906091\n",
      "LOSS: 39655.4898889\n",
      "LOSS: 39655.4892193\n",
      "LOSS: 39655.4885969\n",
      "LOSS: 39655.4880182\n",
      "LOSS: 39655.4874803\n",
      "LOSS: 39655.4869801\n",
      "LOSS: 39655.4865152\n",
      "LOSS: 39655.4860829\n",
      "LOSS: 39655.4856811\n",
      "LOSS: 39655.4853074\n",
      "LOSS: 39655.4849601\n",
      "LOSS: 39655.4846372\n",
      "LOSS: 39655.4843369\n",
      "LOSS: 39655.4840578\n",
      "LOSS: 39655.4837983\n",
      "LOSS: 39655.483557\n",
      "LOSS: 39655.4833327\n",
      "LOSS: 39655.4831242\n",
      "LOSS: 39655.4829304\n",
      "LOSS: 39655.4827501\n",
      "LOSS: 39655.4825824\n",
      "LOSS: 39655.4824264\n",
      "LOSS: 39655.4822812\n",
      "LOSS: 39655.4821462\n",
      "LOSS: 39655.4820207\n",
      "LOSS: 39655.4819038\n",
      "LOSS: 39655.4817952\n",
      "LOSS: 39655.4816941\n",
      "LOSS: 39655.4816001\n",
      "Lamda:  4.96824082422 RMSE:  1.7055519701 Val RMSE: 1.96649235706\n",
      "Lamda:  2.48412041211\n",
      "LOSS: 33440.8628759\n",
      "LOSS: 33305.3543699\n",
      "LOSS: 33276.8405359\n",
      "LOSS: 33266.0234323\n",
      "LOSS: 33260.3398015\n",
      "LOSS: 33256.2075816\n",
      "LOSS: 33252.5817545\n",
      "LOSS: 33249.2034079\n",
      "LOSS: 33245.7881723\n",
      "LOSS: 33242.2824918\n",
      "LOSS: 33238.6761328\n",
      "LOSS: 33234.9953938\n",
      "LOSS: 33231.3336935\n",
      "LOSS: 33227.6767503\n",
      "LOSS: 33224.059418\n",
      "LOSS: 33220.5079419\n",
      "LOSS: 33217.0450813\n",
      "LOSS: 33213.6889601\n",
      "LOSS: 33210.4515499\n",
      "LOSS: 33207.3389463\n",
      "LOSS: 33204.3593009\n",
      "LOSS: 33201.5175113\n",
      "LOSS: 33198.8146401\n",
      "LOSS: 33196.2496762\n",
      "LOSS: 33193.8214463\n",
      "LOSS: 33191.5233021\n",
      "LOSS: 33189.351023\n",
      "LOSS: 33187.2997715\n",
      "LOSS: 33185.3646133\n",
      "LOSS: 33183.5398058\n",
      "LOSS: 33181.8195011\n",
      "LOSS: 33180.1976541\n",
      "LOSS: 33178.6685521\n",
      "LOSS: 33177.2261547\n",
      "LOSS: 33175.8644185\n",
      "LOSS: 33174.5775533\n",
      "LOSS: 33173.3598858\n",
      "LOSS: 33172.2063559\n",
      "LOSS: 33171.1123104\n",
      "LOSS: 33170.0736486\n",
      "LOSS: 33169.0865939\n",
      "LOSS: 33168.1476701\n",
      "LOSS: 33167.2538752\n",
      "LOSS: 33166.4025791\n",
      "LOSS: 33165.5916101\n",
      "LOSS: 33164.818889\n",
      "LOSS: 33164.0826416\n",
      "LOSS: 33163.3809268\n",
      "LOSS: 33162.7119597\n",
      "LOSS: 33162.0741697\n",
      "LOSS: 33161.4661104\n",
      "LOSS: 33160.8864487\n",
      "LOSS: 33160.3339735\n",
      "LOSS: 33159.8075124\n",
      "LOSS: 33159.305973\n",
      "LOSS: 33158.828285\n",
      "LOSS: 33158.3733908\n",
      "LOSS: 33157.9402353\n",
      "LOSS: 33157.5277632\n",
      "LOSS: 33157.1347557\n",
      "LOSS: 33156.7603901\n",
      "LOSS: 33156.4037674\n",
      "LOSS: 33156.0640132\n",
      "LOSS: 33155.7402942\n",
      "LOSS: 33155.4318178\n",
      "LOSS: 33155.1377734\n",
      "LOSS: 33154.8574234\n",
      "LOSS: 33154.5900827\n",
      "LOSS: 33154.3351508\n",
      "LOSS: 33154.0920727\n",
      "LOSS: 33153.8602917\n",
      "LOSS: 33153.6392347\n",
      "LOSS: 33153.4283435\n",
      "LOSS: 33153.2271114\n",
      "LOSS: 33153.0350898\n",
      "LOSS: 33152.8518517\n",
      "LOSS: 33152.6769988\n",
      "LOSS: 33152.5101591\n",
      "LOSS: 33152.3509687\n",
      "LOSS: 33152.1990715\n",
      "LOSS: 33152.0541324\n",
      "LOSS: 33151.9158347\n",
      "LOSS: 33151.7838785\n",
      "LOSS: 33151.6579784\n",
      "LOSS: 33151.5378615\n",
      "LOSS: 33151.4232717\n",
      "LOSS: 33151.3139612\n",
      "LOSS: 33151.2096777\n",
      "LOSS: 33151.1102135\n",
      "LOSS: 33151.0153355\n",
      "LOSS: 33150.9248262\n",
      "LOSS: 33150.8384871\n",
      "LOSS: 33150.756127\n",
      "LOSS: 33150.6775653\n",
      "LOSS: 33150.6026309\n",
      "LOSS: 33150.5311626\n",
      "LOSS: 33150.4630075\n",
      "LOSS: 33150.3980199\n",
      "LOSS: 33150.3360589\n",
      "LOSS: 33150.2769865\n",
      "LOSS: 33150.2205387\n",
      "LOSS: 33150.1666404\n",
      "LOSS: 33150.1152179\n",
      "LOSS: 33150.0661789\n",
      "LOSS: 33150.0194246\n",
      "LOSS: 33149.9748553\n",
      "LOSS: 33149.9323728\n",
      "LOSS: 33149.8918815\n",
      "LOSS: 33149.8532889\n",
      "LOSS: 33149.8165053\n",
      "LOSS: 33149.781445\n",
      "LOSS: 33149.7480298\n",
      "LOSS: 33149.7161802\n",
      "LOSS: 33149.6858211\n",
      "LOSS: 33149.656881\n",
      "LOSS: 33149.6292926\n",
      "LOSS: 33149.6029919\n",
      "LOSS: 33149.5779159\n",
      "LOSS: 33149.5540054\n",
      "LOSS: 33149.531205\n",
      "LOSS: 33149.509462\n",
      "LOSS: 33149.4887271\n",
      "LOSS: 33149.468954\n",
      "LOSS: 33149.4500982\n",
      "LOSS: 33149.4321173\n",
      "LOSS: 33149.4149704\n",
      "LOSS: 33149.3986188\n",
      "LOSS: 33149.383025\n",
      "LOSS: 33149.3681537\n",
      "LOSS: 33149.3539708\n",
      "LOSS: 33149.3404442\n",
      "LOSS: 33149.327543\n",
      "LOSS: 33149.315238\n",
      "LOSS: 33149.3035015\n",
      "LOSS: 33149.292307\n",
      "LOSS: 33149.2816296\n",
      "LOSS: 33149.2714454\n",
      "LOSS: 33149.2617316\n",
      "LOSS: 33149.2524669\n",
      "LOSS: 33149.2436305\n",
      "LOSS: 33149.2352029\n",
      "LOSS: 33149.2271654\n",
      "LOSS: 33149.2195001\n",
      "LOSS: 33149.21219\n",
      "LOSS: 33149.2052188\n",
      "LOSS: 33149.198571\n",
      "LOSS: 33149.1922316\n",
      "LOSS: 33149.1861865\n",
      "LOSS: 33149.180422\n",
      "LOSS: 33149.1749252\n",
      "LOSS: 33149.1696836\n",
      "LOSS: 33149.1646855\n",
      "LOSS: 33149.1599194\n",
      "LOSS: 33149.1553747\n",
      "LOSS: 33149.151041\n",
      "LOSS: 33149.1469085\n",
      "LOSS: 33149.1429679\n",
      "LOSS: 33149.1392101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 33149.1356267\n",
      "LOSS: 33149.1322096\n",
      "LOSS: 33149.128951\n",
      "LOSS: 33149.1258435\n",
      "LOSS: 33149.1228802\n",
      "LOSS: 33149.1200542\n",
      "LOSS: 33149.1173593\n",
      "LOSS: 33149.1147894\n",
      "LOSS: 33149.1123385\n",
      "LOSS: 33149.1100013\n",
      "LOSS: 33149.1077724\n",
      "LOSS: 33149.1056468\n",
      "LOSS: 33149.1036198\n",
      "LOSS: 33149.1016867\n",
      "LOSS: 33149.0998432\n",
      "LOSS: 33149.0980851\n",
      "LOSS: 33149.0964086\n",
      "LOSS: 33149.0948097\n",
      "LOSS: 33149.093285\n",
      "LOSS: 33149.091831\n",
      "LOSS: 33149.0904443\n",
      "LOSS: 33149.089122\n",
      "LOSS: 33149.087861\n",
      "LOSS: 33149.0866584\n",
      "LOSS: 33149.0855116\n",
      "LOSS: 33149.0844179\n",
      "LOSS: 33149.083375\n",
      "LOSS: 33149.0823804\n",
      "LOSS: 33149.081432\n",
      "LOSS: 33149.0805275\n",
      "LOSS: 33149.0796649\n",
      "LOSS: 33149.0788424\n",
      "LOSS: 33149.078058\n",
      "LOSS: 33149.07731\n",
      "LOSS: 33149.0765967\n",
      "LOSS: 33149.0759164\n",
      "LOSS: 33149.0752678\n",
      "LOSS: 33149.0746491\n",
      "LOSS: 33149.0740592\n",
      "LOSS: 33149.0734965\n",
      "LOSS: 33149.0729599\n",
      "LOSS: 33149.0724481\n",
      "LOSS: 33149.0719601\n",
      "LOSS: 33149.0714946\n",
      "LOSS: 33149.0710507\n",
      "LOSS: 33149.0706274\n",
      "LOSS: 33149.0702236\n",
      "LOSS: 33149.0698386\n",
      "LOSS: 33149.0694714\n",
      "LOSS: 33149.0691212\n",
      "LOSS: 33149.0687873\n",
      "LOSS: 33149.0684688\n",
      "LOSS: 33149.0681651\n",
      "LOSS: 33149.0678754\n",
      "LOSS: 33149.0675992\n",
      "LOSS: 33149.0673358\n",
      "LOSS: 33149.0670845\n",
      "LOSS: 33149.066845\n",
      "LOSS: 33149.0666165\n",
      "LOSS: 33149.0663986\n",
      "LOSS: 33149.0661908\n",
      "LOSS: 33149.0659927\n",
      "LOSS: 33149.0658037\n",
      "LOSS: 33149.0656235\n",
      "LOSS: 33149.0654516\n",
      "LOSS: 33149.0652878\n",
      "LOSS: 33149.0651315\n",
      "LOSS: 33149.0649824\n",
      "LOSS: 33149.0648403\n",
      "LOSS: 33149.0647048\n",
      "LOSS: 33149.0645755\n",
      "LOSS: 33149.0644523\n",
      "LOSS: 33149.0643348\n",
      "LOSS: 33149.0642227\n",
      "LOSS: 33149.0641159\n",
      "LOSS: 33149.064014\n",
      "LOSS: 33149.0639168\n",
      "Lamda:  2.48412041211 RMSE:  1.58009408225 Val RMSE: 1.94736378647\n",
      "Lamda:  1.24206020605\n",
      "LOSS: 28563.9161285\n",
      "LOSS: 28471.2633984\n",
      "LOSS: 28445.5406683\n",
      "LOSS: 28434.7018352\n",
      "LOSS: 28429.2615311\n",
      "LOSS: 28425.9938864\n",
      "LOSS: 28423.6803019\n",
      "LOSS: 28421.848238\n",
      "LOSS: 28420.268341\n",
      "LOSS: 28418.7913943\n",
      "LOSS: 28417.3448149\n",
      "LOSS: 28415.8975831\n",
      "LOSS: 28414.4382479\n",
      "LOSS: 28412.9625892\n",
      "LOSS: 28411.4725119\n",
      "LOSS: 28409.9715725\n",
      "LOSS: 28408.465107\n",
      "LOSS: 28406.9595763\n",
      "LOSS: 28405.4619782\n",
      "LOSS: 28403.9793686\n",
      "LOSS: 28402.5181061\n",
      "LOSS: 28401.0839866\n",
      "LOSS: 28399.6823346\n",
      "LOSS: 28398.3172487\n",
      "LOSS: 28396.9920295\n",
      "LOSS: 28395.7088313\n",
      "LOSS: 28394.4695728\n",
      "LOSS: 28393.275494\n",
      "LOSS: 28392.1270308\n",
      "LOSS: 28391.0240784\n",
      "LOSS: 28389.9660703\n",
      "LOSS: 28388.952105\n",
      "LOSS: 28387.9810453\n",
      "LOSS: 28387.051536\n",
      "LOSS: 28386.1621821\n",
      "LOSS: 28385.3114563\n",
      "LOSS: 28384.4978816\n",
      "LOSS: 28383.720102\n",
      "LOSS: 28382.976286\n",
      "LOSS: 28382.2647688\n",
      "LOSS: 28381.5839902\n",
      "LOSS: 28380.9325624\n",
      "LOSS: 28380.309096\n",
      "LOSS: 28379.7121884\n",
      "LOSS: 28379.1403912\n",
      "LOSS: 28378.5924735\n",
      "LOSS: 28378.0673118\n",
      "LOSS: 28377.5638647\n",
      "LOSS: 28377.081006\n",
      "LOSS: 28376.6176778\n",
      "LOSS: 28376.17292\n",
      "LOSS: 28375.7458593\n",
      "LOSS: 28375.335657\n",
      "LOSS: 28374.941527\n",
      "LOSS: 28374.5627414\n",
      "LOSS: 28374.1986257\n",
      "LOSS: 28373.8485532\n",
      "LOSS: 28373.5119351\n",
      "LOSS: 28373.1881608\n",
      "LOSS: 28372.8769167\n",
      "LOSS: 28372.5780218\n",
      "LOSS: 28372.2909532\n",
      "LOSS: 28372.0153193\n",
      "LOSS: 28371.7507707\n",
      "LOSS: 28371.4969612\n",
      "LOSS: 28371.2535187\n",
      "LOSS: 28371.0200311\n",
      "LOSS: 28370.7960959\n",
      "LOSS: 28370.5813185\n",
      "LOSS: 28370.3752762\n",
      "LOSS: 28370.1777655\n",
      "LOSS: 28369.98845\n",
      "LOSS: 28369.8069665\n",
      "LOSS: 28369.6329451\n",
      "LOSS: 28369.4660669\n",
      "LOSS: 28369.3060278\n",
      "LOSS: 28369.1525364\n",
      "LOSS: 28369.0053161\n",
      "LOSS: 28368.8641034\n",
      "LOSS: 28368.7286498\n",
      "LOSS: 28368.5987186\n",
      "LOSS: 28368.4740781\n",
      "LOSS: 28368.3545043\n",
      "LOSS: 28368.2397899\n",
      "LOSS: 28368.1297351\n",
      "LOSS: 28368.024142\n",
      "LOSS: 28367.9228628\n",
      "LOSS: 28367.8256467\n",
      "LOSS: 28367.7322956\n",
      "LOSS: 28367.6426309\n",
      "LOSS: 28367.5564792\n",
      "LOSS: 28367.473675\n",
      "LOSS: 28367.394065\n",
      "LOSS: 28367.3175093\n",
      "LOSS: 28367.2438812\n",
      "LOSS: 28367.1730651\n",
      "LOSS: 28367.1049534\n",
      "LOSS: 28367.0394476\n",
      "LOSS: 28366.9764553\n",
      "LOSS: 28366.915888\n",
      "LOSS: 28366.8576602\n",
      "LOSS: 28366.8016901\n",
      "LOSS: 28366.7478985\n",
      "LOSS: 28366.6962054\n",
      "LOSS: 28366.6465334\n",
      "LOSS: 28366.5988079\n",
      "LOSS: 28366.5529565\n",
      "LOSS: 28366.5089077\n",
      "LOSS: 28366.4665923\n",
      "LOSS: 28366.4259443\n",
      "LOSS: 28366.386898\n",
      "LOSS: 28366.3493921\n"
     ]
    }
   ],
   "source": [
    "model.chooseCorrectLamda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEKCAYAAADuEgmxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n++BvoKkeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3aa29e3410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model.trainlamda, model.trainrmse)\n",
    "plt.plot(model.vallamda, model.valrmse)\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Lamda')\n",
    "ax = plt.gca()\n",
    "ax.invert_xaxis()\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "#plt.savefig('RMSEvsLamda.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEKCAYAAAAvlUMdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqdu/nAoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3aa29e3110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1948, 1949]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model.NonZero)\n",
    "plt.ylabel('Non Zero')\n",
    "plt.xlabel('Iterations')\n",
    "#plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "print model.NonZero\n",
    "#plt.savefig('NonZeroElements.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.saveModel('savedModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.97577343259\n",
      "82.1858902626\n",
      "[ 91.62866941  92.80029715  87.49764054 ...,  91.75573319  87.98896204\n",
      "  90.96964361]\n",
      "[92 90 86 ..., 92 89 88]\n"
     ]
    }
   ],
   "source": [
    "print rmse(model.predict(valX), valY)\n",
    "print rmse(X.transpose() * model.W, valY)\n",
    "print model.predict(valX)\n",
    "print valY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 105.71206656   85.54415933   87.60385796 ...,   87.82942353   87.01643808\n",
      "   85.78716094]\n"
     ]
    }
   ],
   "source": [
    "testXDF = pd.read_csv('Data/testData.txt', names = ['instanceID', 'featureID', 'value'], sep=' ')\n",
    "testX = csr_matrix((valXDF['value'], (valXDF['featureID'], valXDF['instanceID'])))\n",
    "testPredicted = model.predict(testX)\n",
    "print testPredicted\n",
    "np.savetxt(\"out.csv\", testPredicted, delimiter=\",\")\n",
    "#testPredicted.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixture\n",
    "bigData = (X + valX) / 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
